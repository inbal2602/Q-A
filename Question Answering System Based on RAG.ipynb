{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T08:45:50.761960Z",
     "start_time": "2025-03-06T08:45:24.264919Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing your question...\n",
      "\n",
      "Q: what types of AI there are?\n",
      "ğŸ“Œ Intent: Clarification/Definition\n",
      "ğŸ“ Entities: {'locations': [], 'people': [], 'organizations': [], 'dates': [], 'numbers': []}\n",
      "ğŸ“Š Sentiment Analysis: [{'label': '3 stars', 'score': 0.2728259563446045}]\n",
      "ğŸ” AI Document:\n",
      "Today, \n",
      "AI is integrated into numerous industries, transforming the way we interact with technology. AI can be categorized into three main types: \n",
      "1. Narrow AI (Weak AI) â€“ AI systems designed for specific tasks, such as virtual \n",
      "assistants and recommendation systems. 2. General AI (Strong AI) â€“ Hypothetical AI that possesses human-like cognitive abilities \n",
      "and can perform any intellectual task that a human can do. 3. Super AI â€“ A theoretical concept where AI surpasses human intelligence in all aspects. Despite its rapid advancements, AI still faces challenges, such as ethical considerations, bias in \n",
      "algorithms, and the need for large datasets for training.\n",
      "\n",
      "ğŸ® Gamification Document:\n",
      "No relevant information found in the Gamification Document.\n",
      "\n",
      "ğŸ“‚ External Document:\n",
      "No external file provided.\n",
      "\n",
      "Exiting...\n",
      "\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "import pymupdf  \n",
    "import nltk\n",
    "import torch\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# ×˜×¢×™× ×ª ×”××•×“×œ ×œ×™×¦×™×¨×ª Embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# ×˜×•×¢×Ÿ ××ª ×”××•×“×œ×™× ×©×œ Hugging Face ×•-Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
    "\n",
    "# ××•×“×œ ×œ× ×™×ª×•×— ×¡× ×˜×™×× ×˜×™× ×¢×‘×•×¨ ×˜×§×¡×˜×™×\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "#  ××•×“×œ Zero-Shot Classification ×œ×¡×™×•×•×’ ×›×•×•× ×”\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# ×¨×©×™××ª ×§×˜×’×•×¨×™×•×ª ×œ×¡×™×•×•×’ ×”×›×•×•× ×” ×©×œ ×”×©××œ×”\n",
    "INTENT_CATEGORIES = [\n",
    "    \"Clarification/Definition\",\n",
    "    \"Opinions/Discussion\",\n",
    "    \"Instructions\",\n",
    "    \"Troubleshooting\",\n",
    "    \"Facts\"\n",
    "]\n",
    "\n",
    "# ××–×”×” ××ª ×”×›×•×•× ×” ×©×œ ×”×©××œ×” ×©×”××©×ª××© ×©×•××œ\n",
    "def classify_intent_nlp(question):\n",
    "    result = classifier(question, INTENT_CATEGORIES)\n",
    "    best_category = result[\"labels\"][0]  \n",
    "    return best_category\n",
    "\n",
    "# ×¤×•× ×§×¦×™×” ×œ×§×¨×™××ª ×˜×§×¡×˜ ××ª×•×š ×§×•×‘×¥ PDF\n",
    "def load_pdf(file_path):\n",
    "    text = \"\"\n",
    "    doc = pymupdf.open(file_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# ×—×œ×•×§×ª ×”×˜×§×¡×˜ ×œ×—×œ×§×™×\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += len(sentence)\n",
    "\n",
    "        if current_length > chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ×‘×•×“×§ ×× ××™×œ×” ××•×¤×™×¢×” ×‘×”×§×©×¨ ×˜×›× ×•×œ×•×’×™\n",
    "def appears_in_tech_context(word, sentence):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == word.lower():\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                for child in token.children:\n",
    "                    if child.pos_ in {\"VERB\", \"ADJ\", \"ADV\"}:  \n",
    "                        return True\n",
    "\n",
    "            for ancestor in token.ancestors:\n",
    "                if ancestor.pos_ == \"VERB\":\n",
    "                    return True  \n",
    "\n",
    "            if token.dep_ in {\"dobj\", \"pobj\", \"attr\", \"nsubj\"}:\n",
    "                return True\n",
    "\n",
    "    return False  \n",
    "\n",
    "# ××–×”×” ×™×©×•×™×•×ª ×‘×˜×§×¡×˜ ×•××ª×§×Ÿ ×©×’×™××•×ª ×–×™×”×•×™\n",
    "def extract_entities_combined(text):\n",
    "    entities = {\n",
    "        \"locations\": [], \"people\": [], \"organizations\": [],\n",
    "        \"dates\": [], \"numbers\": [], }\n",
    "\n",
    "    # ×§×‘×œ×ª ×™×©×•×™×•×ª ×¢× Hugging Face\n",
    "    ner_results = ner_pipeline(text)\n",
    "\n",
    "    # ×–×™×”×•×™ ×™×©×•×™×•×ª ×¢× Spacy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # ×ª×™×§×•×Ÿ ×™×©×•×™×•×ª ×©×–×•×”×• ×‘×¦×•×¨×” ×©×’×•×™×”\n",
    "    for entity in ner_results:\n",
    "        entity_text = entity['word'].strip()\n",
    "        entity_label = entity['entity_group']\n",
    "\n",
    "        # ×“×™×œ×•×’ ×¢×œ ×™×©×•×™×•×ª ×©×–×•×”×• ×‘×¦×•×¨×” ×©×’×•×™×”\n",
    "        if is_misclassified_entity(entity_text, entity_label, text):\n",
    "            continue\n",
    "\n",
    "            # ×”×•×¡×¤×ª ×”×™×©×•×ª ×”××ª×•×§× ×ª ×œ×¤×™ ×”×§×˜×’×•×¨×™×” ×”××ª××™××”\n",
    "        if entity_label == \"LOC\":\n",
    "            entities[\"locations\"].append(entity_text)\n",
    "        elif entity_label == \"PER\":\n",
    "            entities[\"people\"].append(entity_text)\n",
    "        elif entity_label == \"ORG\":\n",
    "            entities[\"organizations\"].append(entity_text)\n",
    "        elif entity_label == \"MISC\":\n",
    "            if any(char.isdigit() for char in entity_text):\n",
    "                entities[\"numbers\"].append(entity_text)\n",
    "            else:\n",
    "                entities[\"organizations\"].append(entity_text)\n",
    "\n",
    "    # ×ª×™×§×•×Ÿ ×•×”×•×¡×¤×ª ×™×©×•×™×•×ª × ×•×¡×¤×•×ª ×-Spacy\n",
    "    for ent in doc.ents:\n",
    "        entity_text = ent.text.strip()\n",
    "        entity_label = ent.label_\n",
    "\n",
    "        if is_misclassified_entity(entity_text, entity_label, text):\n",
    "            continue \n",
    "\n",
    "        if entity_label == \"GPE\":\n",
    "            entities[\"locations\"].append(entity_text)\n",
    "        elif entity_label == \"PERSON\":\n",
    "            entities[\"people\"].append(entity_text)\n",
    "        elif entity_label in {\"ORG\", \"FAC\"}:\n",
    "            entities[\"organizations\"].append(entity_text)\n",
    "        elif entity_label == \"DATE\":\n",
    "            entities[\"dates\"].append(entity_text)\n",
    "        elif entity_label in {\"CARDINAL\", \"QUANTITY\"}:\n",
    "            entities[\"numbers\"].append(entity_text)\n",
    "\n",
    "    # × ×™×§×•×™ ×›×¤×™×œ×•×™×•×ª ×•×ª×™×§×•×Ÿ ×©××•×ª\n",
    "    entities[\"organizations\"] = clean_organizations(entities[\"organizations\"])\n",
    "\n",
    "    # × ×™×§×•×™ ×›×¤×™×œ×•×™×•×ª ×‘×›×œ ×”×¨×©×™××•×ª\n",
    "    for key in entities:\n",
    "        entities[key] = list(set(entities[key]))\n",
    "\n",
    "    return entities\n",
    "\n",
    "# ××–×”×” ×× ×™×©×•×ª ×¡×•×•×’×” ×‘×¦×•×¨×” ×œ× × ×›×•× ×”\n",
    "def is_misclassified_entity(entity_text, entity_label, context):\n",
    "    doc = nlp(context)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == entity_text.lower():\n",
    "            if entity_label == \"ORG\" and is_tech_term(entity_text):\n",
    "                return True\n",
    "\n",
    "            if entity_label == \"GPE\" and is_not_real_location(entity_text):\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# ××–×”×” ×× ××™×œ×” ×”×™× ××•× ×— ×˜×›× ×•×œ×•×’×™ ×¢×œ ×‘×¡×™×¡ Embeddings\n",
    "def is_tech_term(word):\n",
    "    embedding_vector = embedding_model.encode(word, convert_to_tensor=True)\n",
    "    similarity_score = embedding_model.similarity(embedding_vector, embedding_model.encode(\"technology\"))\n",
    "    return similarity_score > 0.6  \n",
    "\n",
    "#  ×¤×•× ×§×¦×™×” ×œ×‘×“×•×§ ×× ××™×§×•× ×”×•× ×œ× ×‘×××ª ××™×§×•×\n",
    "def is_not_real_location(word):\n",
    "    embedding_vector = embedding_model.encode(word, convert_to_tensor=True)\n",
    "    similarity_score = embedding_model.similarity(embedding_vector, embedding_model.encode(\"geographic place\"))\n",
    "\n",
    "    return similarity_score < 0.5\n",
    "\n",
    "# ×× ×§×” ×©××•×ª ×™×©×•×™×•×ª ×¢×œ ×™×“×™ ×”×¡×¨×ª ××™×œ×™× ×œ× ×¨×œ×•×•× ×˜×™×•×ª\n",
    "def normalize_entity_name(entity_name):\n",
    "    doc = nlp(entity_name) \n",
    "    tokens = [token.text for token in doc if token.dep_ != \"det\"] \n",
    "    cleaned_name = \" \".join(tokens).strip() \n",
    "    return cleaned_name\n",
    "\n",
    "# ×× ×§×” ×©××•×ª ××¨×’×•× ×™× ×•××•× ×¢ ×›×¤×™×œ×•×™×•×ª\n",
    "def clean_organizations(organization_list):\n",
    "    cleaned_orgs = set()  \n",
    "\n",
    "    for org in organization_list:\n",
    "        normalized_org = normalize_entity_name(org)  \n",
    "        cleaned_orgs.add(normalized_org)\n",
    "\n",
    "    return list(cleaned_orgs)\n",
    "\n",
    "# ××‘×¦×¢ ×—×™×‘×•×¨ ×©×œ ×™×©×•×™×•×ª ××¨×•×‘×•×ª ××™×œ×™×\n",
    "def merge_multiword_entities(entity_list):\n",
    "    merged_entities = []\n",
    "    skip_next = False\n",
    "\n",
    "    for i in range(len(entity_list) - 1):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        current_entity = entity_list[i]\n",
    "        next_entity = entity_list[i + 1]\n",
    "\n",
    "        #  ×× ×™×¢×ª ×—×™×‘×•×¨ ×©×œ ××•×ª×’×™× × ×¤×¨×“×™× ×™×—×“\n",
    "        if len(current_entity.split()) == 1 and len(next_entity.split()) == 1:\n",
    "            merged_entities.append(current_entity)\n",
    "            merged_entities.append(next_entity)\n",
    "            skip_next = True\n",
    "            continue\n",
    "\n",
    "        #  ×—×™×‘×•×¨ ×™×©×•×™×•×ª ××¨×•×‘×•×ª ××™×œ×™×\n",
    "        if current_entity.istitle() and next_entity.istitle():\n",
    "            combined_entity = f\"{current_entity} {next_entity}\"\n",
    "            if combined_entity not in merged_entities:\n",
    "                merged_entities.append(combined_entity)\n",
    "            skip_next = True\n",
    "        else:\n",
    "            if current_entity not in merged_entities:\n",
    "                merged_entities.append(current_entity)\n",
    "\n",
    "    if not skip_next and entity_list[-1] not in merged_entities:\n",
    "        merged_entities.append(entity_list[-1])\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "# ×‘×•×“×§ ×”×× ×”××™×§×•× ×©× ×™×ª×Ÿ ×‘×©××œ×” ××›×Ÿ ××™×§×•× ×××™×ª×™\n",
    "def is_ambiguous_location(word, sentence):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    tech_terms = {\"AI\", \"machine learning\", \"data\", \"software\", \"algorithm\", \"platform\", \"deep learning\"}\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == word.lower():\n",
    "            surrounding_words = {t.text.lower() for t in token.sent}\n",
    "            if tech_terms & surrounding_words:\n",
    "                return True \n",
    "\n",
    "    return False  \n",
    "\n",
    "#  ×˜×¢×™× ×ª ×§×•×‘×¦×™ ×”×˜×§×¡×˜\n",
    "ai_text = load_pdf(\"Artificial Intelligence and Its Applications.pdf\")\n",
    "gamification_text = load_pdf(\"History of Gamification and Its Role in the Educational Process.pdf\")\n",
    "\n",
    "# ×ª××™×›×” ×‘×§×•×‘×¥ ×—×™×¦×•× ×™ ×©×”××©×ª××© ×™×‘×—×¨\n",
    "external_file = input(\"Enter the path for an external file (or press Enter to skip): \")\n",
    "external_uploaded = bool(external_file)\n",
    "\n",
    "if external_uploaded:\n",
    "    external_text = load_pdf(external_file)\n",
    "    external_chunks = chunk_text(external_text)\n",
    "    external_embeddings = embedding_model.encode(external_chunks, convert_to_tensor=True)\n",
    "else:\n",
    "    external_chunks = []\n",
    "    external_embeddings = None\n",
    "\n",
    "# ×™×¦×™×¨×ª Chunks ×œ×›×œ ××¡××š\n",
    "ai_chunks = chunk_text(ai_text)\n",
    "gamification_chunks = chunk_text(gamification_text)\n",
    "\n",
    "# ×™×¦×™×¨×ª Embeddings ×œ×›×œ ×¦'×× ×§\n",
    "ai_embeddings = embedding_model.encode(ai_chunks, convert_to_tensor=True)\n",
    "gamification_embeddings = embedding_model.encode(gamification_chunks, convert_to_tensor=True)\n",
    "\n",
    "# ×—×™×¤×•×© ××™×“×¢ ×¨×œ×•×•× ×˜×™ ×‘××¡××›×™×\n",
    "def search_relevant_text(question, embeddings, chunks, doc_name=\"Document\"):\n",
    "    if embeddings is None or len(chunks) == 0:\n",
    "        return f\"No relevant information found in the {doc_name}.\"\n",
    "\n",
    "    question_embedding = embedding_model.encode([question], convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(question_embedding, embeddings)[0]\n",
    "\n",
    "    best_idx = similarities.argmax().item()\n",
    "    best_score = similarities[best_idx].item()\n",
    "\n",
    "    SIMILARITY_THRESHOLD = 0.4\n",
    "    if best_score < SIMILARITY_THRESHOLD:\n",
    "        return f\"No relevant information found in the {doc_name}.\"\n",
    "\n",
    "    return chunks[best_idx]\n",
    "\n",
    "# ×œ×•×œ××ª ×”×©××œ×•×ª\n",
    "while True:\n",
    "    user_question = input('What can I help you with? (Type \"exit\" to quit) ')\n",
    "    if user_question.lower() == 'exit':\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "\n",
    "    print('\\nProcessing your question...\\n')\n",
    "\n",
    "    # ×¡×™×•×•×’ ×›×•×•× ×” ×‘×××¦×¢×•×ª facebook/bart-large-mnli\n",
    "    question_intent = classify_intent_nlp(user_question)\n",
    "\n",
    "    # ×–×™×”×•×™ ×™×©×•×™×•×ª ×‘×˜×§×¡×˜\n",
    "    entities = extract_entities_combined(user_question)\n",
    "\n",
    "    # × ×™×ª×•×— ×¡× ×˜×™×× ×˜\n",
    "    sentiment_result = sentiment_pipeline(user_question[:512])\n",
    "\n",
    "    #  ×—×™×¤×•×© ××™×“×¢ ×¨×œ×•×•× ×˜×™ ×‘××¡××›×™×\n",
    "    ai_answer = search_relevant_text(user_question, ai_embeddings, ai_chunks, \"AI Document\")\n",
    "    gamification_answer = search_relevant_text(user_question, gamification_embeddings, gamification_chunks, \"Gamification Document\")\n",
    "\n",
    "    # ×—×™×¤×•×© ××™×“×¢ ×¨×œ×•×•× ×˜×™ ×‘×§×•×‘×¥ ×”×—×™×¦×•× ×™ ×× ×§×™×™×\n",
    "    if external_uploaded:\n",
    "        external_answer = search_relevant_text(user_question, external_embeddings, external_chunks, \"External Document\")\n",
    "    else:\n",
    "        external_answer = \"No external file provided.\"\n",
    "\n",
    "    # ×”×“×¤×¡×ª ×”×ª×•×¦××•×ª\n",
    "    print(f'Q: {user_question}')\n",
    "    print(f'ğŸ“Œ Intent: {question_intent}')\n",
    "    print(f'ğŸ“ Entities: {entities}')\n",
    "    print(f'ğŸ“Š Sentiment Analysis: {sentiment_result}')\n",
    "    print(f'ğŸ” AI Document:\\n{ai_answer}\\n')\n",
    "    print(f'ğŸ® Gamification Document:\\n{gamification_answer}\\n')\n",
    "    print(f'ğŸ“‚ External Document:\\n{external_answer}\\n')\n",
    "\n",
    "print(\"\\n---------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

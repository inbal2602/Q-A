{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T08:45:50.761960Z",
     "start_time": "2025-03-06T08:45:24.264919Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing your question...\n",
      "\n",
      "Q: what types of AI there are?\n",
      "📌 Intent: Clarification/Definition\n",
      "📍 Entities: {'locations': [], 'people': [], 'organizations': [], 'dates': [], 'numbers': []}\n",
      "📊 Sentiment Analysis: [{'label': '3 stars', 'score': 0.2728259563446045}]\n",
      "🔎 AI Document:\n",
      "Today, \n",
      "AI is integrated into numerous industries, transforming the way we interact with technology. AI can be categorized into three main types: \n",
      "1. Narrow AI (Weak AI) – AI systems designed for specific tasks, such as virtual \n",
      "assistants and recommendation systems. 2. General AI (Strong AI) – Hypothetical AI that possesses human-like cognitive abilities \n",
      "and can perform any intellectual task that a human can do. 3. Super AI – A theoretical concept where AI surpasses human intelligence in all aspects. Despite its rapid advancements, AI still faces challenges, such as ethical considerations, bias in \n",
      "algorithms, and the need for large datasets for training.\n",
      "\n",
      "🎮 Gamification Document:\n",
      "No relevant information found in the Gamification Document.\n",
      "\n",
      "📂 External Document:\n",
      "No external file provided.\n",
      "\n",
      "Exiting...\n",
      "\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "import pymupdf  \n",
    "import nltk\n",
    "import torch\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# טעינת המודל ליצירת Embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# טוען את המודלים של Hugging Face ו-Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
    "\n",
    "# מודל לניתוח סנטימנטים עבור טקסטים\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "#  מודל Zero-Shot Classification לסיווג כוונה\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# רשימת קטגוריות לסיווג הכוונה של השאלה\n",
    "INTENT_CATEGORIES = [\n",
    "    \"Clarification/Definition\",\n",
    "    \"Opinions/Discussion\",\n",
    "    \"Instructions\",\n",
    "    \"Troubleshooting\",\n",
    "    \"Facts\"\n",
    "]\n",
    "\n",
    "# מזהה את הכוונה של השאלה שהמשתמש שואל\n",
    "def classify_intent_nlp(question):\n",
    "    result = classifier(question, INTENT_CATEGORIES)\n",
    "    best_category = result[\"labels\"][0]  \n",
    "    return best_category\n",
    "\n",
    "# פונקציה לקריאת טקסט מתוך קובץ PDF\n",
    "def load_pdf(file_path):\n",
    "    text = \"\"\n",
    "    doc = pymupdf.open(file_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# חלוקת הטקסט לחלקים\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += len(sentence)\n",
    "\n",
    "        if current_length > chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# בודק אם מילה מופיעה בהקשר טכנולוגי\n",
    "def appears_in_tech_context(word, sentence):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == word.lower():\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                for child in token.children:\n",
    "                    if child.pos_ in {\"VERB\", \"ADJ\", \"ADV\"}:  \n",
    "                        return True\n",
    "\n",
    "            for ancestor in token.ancestors:\n",
    "                if ancestor.pos_ == \"VERB\":\n",
    "                    return True  \n",
    "\n",
    "            if token.dep_ in {\"dobj\", \"pobj\", \"attr\", \"nsubj\"}:\n",
    "                return True\n",
    "\n",
    "    return False  \n",
    "\n",
    "# מזהה ישויות בטקסט ומתקן שגיאות זיהוי\n",
    "def extract_entities_combined(text):\n",
    "    entities = {\n",
    "        \"locations\": [], \"people\": [], \"organizations\": [],\n",
    "        \"dates\": [], \"numbers\": [], }\n",
    "\n",
    "    # קבלת ישויות עם Hugging Face\n",
    "    ner_results = ner_pipeline(text)\n",
    "\n",
    "    # זיהוי ישויות עם Spacy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # תיקון ישויות שזוהו בצורה שגויה\n",
    "    for entity in ner_results:\n",
    "        entity_text = entity['word'].strip()\n",
    "        entity_label = entity['entity_group']\n",
    "\n",
    "        # דילוג על ישויות שזוהו בצורה שגויה\n",
    "        if is_misclassified_entity(entity_text, entity_label, text):\n",
    "            continue\n",
    "\n",
    "            # הוספת הישות המתוקנת לפי הקטגוריה המתאימה\n",
    "        if entity_label == \"LOC\":\n",
    "            entities[\"locations\"].append(entity_text)\n",
    "        elif entity_label == \"PER\":\n",
    "            entities[\"people\"].append(entity_text)\n",
    "        elif entity_label == \"ORG\":\n",
    "            entities[\"organizations\"].append(entity_text)\n",
    "        elif entity_label == \"MISC\":\n",
    "            if any(char.isdigit() for char in entity_text):\n",
    "                entities[\"numbers\"].append(entity_text)\n",
    "            else:\n",
    "                entities[\"organizations\"].append(entity_text)\n",
    "\n",
    "    # תיקון והוספת ישויות נוספות מ-Spacy\n",
    "    for ent in doc.ents:\n",
    "        entity_text = ent.text.strip()\n",
    "        entity_label = ent.label_\n",
    "\n",
    "        if is_misclassified_entity(entity_text, entity_label, text):\n",
    "            continue \n",
    "\n",
    "        if entity_label == \"GPE\":\n",
    "            entities[\"locations\"].append(entity_text)\n",
    "        elif entity_label == \"PERSON\":\n",
    "            entities[\"people\"].append(entity_text)\n",
    "        elif entity_label in {\"ORG\", \"FAC\"}:\n",
    "            entities[\"organizations\"].append(entity_text)\n",
    "        elif entity_label == \"DATE\":\n",
    "            entities[\"dates\"].append(entity_text)\n",
    "        elif entity_label in {\"CARDINAL\", \"QUANTITY\"}:\n",
    "            entities[\"numbers\"].append(entity_text)\n",
    "\n",
    "    # ניקוי כפילויות ותיקון שמות\n",
    "    entities[\"organizations\"] = clean_organizations(entities[\"organizations\"])\n",
    "\n",
    "    # ניקוי כפילויות בכל הרשימות\n",
    "    for key in entities:\n",
    "        entities[key] = list(set(entities[key]))\n",
    "\n",
    "    return entities\n",
    "\n",
    "# מזהה אם ישות סווגה בצורה לא נכונה\n",
    "def is_misclassified_entity(entity_text, entity_label, context):\n",
    "    doc = nlp(context)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == entity_text.lower():\n",
    "            if entity_label == \"ORG\" and is_tech_term(entity_text):\n",
    "                return True\n",
    "\n",
    "            if entity_label == \"GPE\" and is_not_real_location(entity_text):\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# מזהה אם מילה היא מונח טכנולוגי על בסיס Embeddings\n",
    "def is_tech_term(word):\n",
    "    embedding_vector = embedding_model.encode(word, convert_to_tensor=True)\n",
    "    similarity_score = embedding_model.similarity(embedding_vector, embedding_model.encode(\"technology\"))\n",
    "    return similarity_score > 0.6  \n",
    "\n",
    "#  פונקציה לבדוק אם מיקום הוא לא באמת מיקום\n",
    "def is_not_real_location(word):\n",
    "    embedding_vector = embedding_model.encode(word, convert_to_tensor=True)\n",
    "    similarity_score = embedding_model.similarity(embedding_vector, embedding_model.encode(\"geographic place\"))\n",
    "\n",
    "    return similarity_score < 0.5\n",
    "\n",
    "# מנקה שמות ישויות על ידי הסרת מילים לא רלוונטיות\n",
    "def normalize_entity_name(entity_name):\n",
    "    doc = nlp(entity_name) \n",
    "    tokens = [token.text for token in doc if token.dep_ != \"det\"] \n",
    "    cleaned_name = \" \".join(tokens).strip() \n",
    "    return cleaned_name\n",
    "\n",
    "# מנקה שמות ארגונים ומונע כפילויות\n",
    "def clean_organizations(organization_list):\n",
    "    cleaned_orgs = set()  \n",
    "\n",
    "    for org in organization_list:\n",
    "        normalized_org = normalize_entity_name(org)  \n",
    "        cleaned_orgs.add(normalized_org)\n",
    "\n",
    "    return list(cleaned_orgs)\n",
    "\n",
    "# מבצע חיבור של ישויות מרובות מילים\n",
    "def merge_multiword_entities(entity_list):\n",
    "    merged_entities = []\n",
    "    skip_next = False\n",
    "\n",
    "    for i in range(len(entity_list) - 1):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        current_entity = entity_list[i]\n",
    "        next_entity = entity_list[i + 1]\n",
    "\n",
    "        #  מניעת חיבור של מותגים נפרדים יחד\n",
    "        if len(current_entity.split()) == 1 and len(next_entity.split()) == 1:\n",
    "            merged_entities.append(current_entity)\n",
    "            merged_entities.append(next_entity)\n",
    "            skip_next = True\n",
    "            continue\n",
    "\n",
    "        #  חיבור ישויות מרובות מילים\n",
    "        if current_entity.istitle() and next_entity.istitle():\n",
    "            combined_entity = f\"{current_entity} {next_entity}\"\n",
    "            if combined_entity not in merged_entities:\n",
    "                merged_entities.append(combined_entity)\n",
    "            skip_next = True\n",
    "        else:\n",
    "            if current_entity not in merged_entities:\n",
    "                merged_entities.append(current_entity)\n",
    "\n",
    "    if not skip_next and entity_list[-1] not in merged_entities:\n",
    "        merged_entities.append(entity_list[-1])\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "# בודק האם המיקום שניתן בשאלה אכן מיקום אמיתי\n",
    "def is_ambiguous_location(word, sentence):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    tech_terms = {\"AI\", \"machine learning\", \"data\", \"software\", \"algorithm\", \"platform\", \"deep learning\"}\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == word.lower():\n",
    "            surrounding_words = {t.text.lower() for t in token.sent}\n",
    "            if tech_terms & surrounding_words:\n",
    "                return True \n",
    "\n",
    "    return False  \n",
    "\n",
    "#  טעינת קובצי הטקסט\n",
    "ai_text = load_pdf(\"Artificial Intelligence and Its Applications.pdf\")\n",
    "gamification_text = load_pdf(\"History of Gamification and Its Role in the Educational Process.pdf\")\n",
    "\n",
    "# תמיכה בקובץ חיצוני שהמשתמש יבחר\n",
    "external_file = input(\"Enter the path for an external file (or press Enter to skip): \")\n",
    "external_uploaded = bool(external_file)\n",
    "\n",
    "if external_uploaded:\n",
    "    external_text = load_pdf(external_file)\n",
    "    external_chunks = chunk_text(external_text)\n",
    "    external_embeddings = embedding_model.encode(external_chunks, convert_to_tensor=True)\n",
    "else:\n",
    "    external_chunks = []\n",
    "    external_embeddings = None\n",
    "\n",
    "# יצירת Chunks לכל מסמך\n",
    "ai_chunks = chunk_text(ai_text)\n",
    "gamification_chunks = chunk_text(gamification_text)\n",
    "\n",
    "# יצירת Embeddings לכל צ'אנק\n",
    "ai_embeddings = embedding_model.encode(ai_chunks, convert_to_tensor=True)\n",
    "gamification_embeddings = embedding_model.encode(gamification_chunks, convert_to_tensor=True)\n",
    "\n",
    "# חיפוש מידע רלוונטי במסמכים\n",
    "def search_relevant_text(question, embeddings, chunks, doc_name=\"Document\"):\n",
    "    if embeddings is None or len(chunks) == 0:\n",
    "        return f\"No relevant information found in the {doc_name}.\"\n",
    "\n",
    "    question_embedding = embedding_model.encode([question], convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(question_embedding, embeddings)[0]\n",
    "\n",
    "    best_idx = similarities.argmax().item()\n",
    "    best_score = similarities[best_idx].item()\n",
    "\n",
    "    SIMILARITY_THRESHOLD = 0.4\n",
    "    if best_score < SIMILARITY_THRESHOLD:\n",
    "        return f\"No relevant information found in the {doc_name}.\"\n",
    "\n",
    "    return chunks[best_idx]\n",
    "\n",
    "# לולאת השאלות\n",
    "while True:\n",
    "    user_question = input('What can I help you with? (Type \"exit\" to quit) ')\n",
    "    if user_question.lower() == 'exit':\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "\n",
    "    print('\\nProcessing your question...\\n')\n",
    "\n",
    "    # סיווג כוונה באמצעות facebook/bart-large-mnli\n",
    "    question_intent = classify_intent_nlp(user_question)\n",
    "\n",
    "    # זיהוי ישויות בטקסט\n",
    "    entities = extract_entities_combined(user_question)\n",
    "\n",
    "    # ניתוח סנטימנט\n",
    "    sentiment_result = sentiment_pipeline(user_question[:512])\n",
    "\n",
    "    #  חיפוש מידע רלוונטי במסמכים\n",
    "    ai_answer = search_relevant_text(user_question, ai_embeddings, ai_chunks, \"AI Document\")\n",
    "    gamification_answer = search_relevant_text(user_question, gamification_embeddings, gamification_chunks, \"Gamification Document\")\n",
    "\n",
    "    # חיפוש מידע רלוונטי בקובץ החיצוני אם קיים\n",
    "    if external_uploaded:\n",
    "        external_answer = search_relevant_text(user_question, external_embeddings, external_chunks, \"External Document\")\n",
    "    else:\n",
    "        external_answer = \"No external file provided.\"\n",
    "\n",
    "    # הדפסת התוצאות\n",
    "    print(f'Q: {user_question}')\n",
    "    print(f'📌 Intent: {question_intent}')\n",
    "    print(f'📍 Entities: {entities}')\n",
    "    print(f'📊 Sentiment Analysis: {sentiment_result}')\n",
    "    print(f'🔎 AI Document:\\n{ai_answer}\\n')\n",
    "    print(f'🎮 Gamification Document:\\n{gamification_answer}\\n')\n",
    "    print(f'📂 External Document:\\n{external_answer}\\n')\n",
    "\n",
    "print(\"\\n---------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
